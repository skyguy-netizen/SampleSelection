{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2b237e-26cd-4c54-89f4-314d15c90814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cae118f-28b6-41e1-bfe3-3ecdec9fae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from the research paper\n",
    "# https://github.com/danielvik/arc_rtpred/blob/main/notebooks_and_code/functions/featurizers.py\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import deepchem as dc\n",
    "import os\n",
    "import subprocess\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "######*\n",
    "######* FUNCTIONS FOR FEATURIZING\n",
    "######*\n",
    "\n",
    "####################################\n",
    "# * Get METLIN SMRT data from remote URL\n",
    "####################################\n",
    "\n",
    "\n",
    "def get_METLIN_data(sampling=5000, only_retained=True):\n",
    "    \"\"\"Function to pull METLIN SMRT data from remote URL and save subset as a csv file\"\"\"\n",
    "    # * pulling data from a URL\n",
    "    url = \"https://figshare.com/ndownloader/files/18130628\"\n",
    "\n",
    "    # * read the data into a pandas dataframe and convert inchi to SMILES\n",
    "    df = pd.read_csv(url, sep=\";\").rename(columns={\"pubchem\": \"id\"})\n",
    "    df[\"mol\"] = [Chem.MolFromInchi(x) for x in df[\"inchi\"]]\n",
    "    df[\"smiles\"] = [\n",
    "        Chem.MolToSmiles(mol) if mol is not None else None for mol in df[\"mol\"]\n",
    "    ]\n",
    "    df = df.dropna(\n",
    "        axis=0\n",
    "    )  # * dropping rows with NaN values as some mols did not convert to SMILES\n",
    "\n",
    "    # * removing non-retained compounds\n",
    "    if only_retained:\n",
    "        df = df[df[\"rt\"] > 200]\n",
    "\n",
    "    # * sampling the data\n",
    "    df_subset = df[[\"id\", \"smiles\", \"rt\"]]\n",
    "\n",
    "    # * outputting the data in a csv file\n",
    "    output_dir = \"../data/metlin_smrt\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, f\"sample_dataset_{sampling}_datapoints.csv\")\n",
    "    df_subset.to_csv(output_path, index=False)\n",
    "    print(\"saved to: \", output_path, \"\\n\")\n",
    "    print(df_subset.head(3))\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "####################################\n",
    "# * ChemAxon-based LogD calculations\n",
    "####################################\n",
    "\n",
    "\n",
    "def LogD_calculations(data, path_to_chemaxon_licence=None, path_to_chemaxon_tools=None):\n",
    "    \"\"\"\n",
    "    Function to calculate LogD using ChemAxon's commandline tool. The function takes a pandas dataframe with a column named 'smiles' and returns a dataframe with the calculated LogD values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ##* these are hardcoded variables for property calculcations\n",
    "    os.environ[\"CHEMAXON_LICENSE_URL\"] = path_to_chemaxon_licence\n",
    "\n",
    "    exe_file_path = os.path.join(\n",
    "        path_to_chemaxon_tools, \"ChemAxon/MarvinSuite/bin/cxcalc\"\n",
    "    )\n",
    "    input_file_path = os.path.join(\n",
    "        path_to_chemaxon_tools, \"smiles_for_chemaxon_logd_calc.smiles\"\n",
    "    )\n",
    "    output_file_path = os.path.join(path_to_chemaxon_tools, \"ChemAxon_LogD_out.csv\")\n",
    "\n",
    "    ##* saving data for the commandline tool to access it\n",
    "    data.to_csv(input_file_path, index=False, header=False)\n",
    "\n",
    "    ##* running the commandline tool\n",
    "    filepath = f'{exe_file_path} -g logd -m weighted -H 7.4 logd -H 7 logd -H 6.5 logd -H 6 logd -H 5.5 logd -H 5 logd -H 4.5 logd -H 4 logd -H 3.5 logd -H 3 logd -H 2.5 logd -H 2 logd -H 1.5 logd -H 1 logd -H 0.5 logd -H 0 \"{input_file_path}\" > \"{output_file_path}\"'\n",
    "    subprocess.run(filepath, shell=True)\n",
    "\n",
    "    ##* reading the output file\n",
    "    logd_desc = pd.read_csv(output_file_path, sep=\"\\t\")\n",
    "    # drop rows with 'logd:FAILED' values\n",
    "    logd_desc = logd_desc[~logd_desc[\"logD[pH=7.4]\"].str.contains(\"FAILED\")]\n",
    "    # drop rows with NA values\n",
    "    logd_desc = logd_desc.dropna(axis=0)\n",
    "\n",
    "    logd_desc = logd_desc.replace(\",\", \".\", regex=True).apply(pd.to_numeric)\n",
    "    logd_desc = pd.concat([data, logd_desc], axis=1).drop([\"id\"], axis=1)\n",
    "\n",
    "    ##* output the descriptors\n",
    "    return logd_desc\n",
    "\n",
    "\n",
    "####################################\n",
    "# * Get all features\n",
    "####################################\n",
    "\n",
    "\n",
    "def get_features(\n",
    "    path_to_data,\n",
    "    feature_dir,\n",
    "    feature_list,\n",
    "    path_to_chemaxon_licence=None,\n",
    "    path_to_chemaxon_tools=None,\n",
    "):\n",
    "    \"\"\"function to get the features from the input feature_list. Features are saved to a csv file in the feature_dir\"\"\"\n",
    "\n",
    "    # ECFP4 arguments\n",
    "    nBits = 2048\n",
    "    radius = 2\n",
    "\n",
    "    data = pd.read_csv(path_to_data)\n",
    "\n",
    "    ###* LOG D Calculations\n",
    "    if \"logD\" in feature_list and path_to_chemaxon_licence is not None:\n",
    "        target_dir = os.path.join(feature_dir, \"logd_calculations\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        ##* the SMILES must be saved in a flat file for the cmdline tool to read them\n",
    "        # subsetting the test data, to save time\n",
    "        data_for_calc = data[[\"smiles\"]].reset_index(drop=True)\n",
    "\n",
    "        print(\"ChemAxon-based LogD calculations - to .CSV\")\n",
    "\n",
    "        desc = LogD_calculations(\n",
    "            data_for_calc,\n",
    "            path_to_chemaxon_licence=path_to_chemaxon_licence,\n",
    "            path_to_chemaxon_tools=path_to_chemaxon_tools,\n",
    "        )\n",
    "\n",
    "        desc = data.merge(desc, on=\"smiles\", how=\"left\")\n",
    "\n",
    "        desc.to_csv(os.path.join(target_dir, \"all_data.csv\"), index=False)\n",
    "\n",
    "        NA_alert = desc.isna().any().any()\n",
    "        if NA_alert:\n",
    "            print(\"*** NA values in:\", target_dir)\n",
    "\n",
    "    ###* ECFP4 Calculations\n",
    "    if \"ecfp4\" in feature_list:\n",
    "        target_dir = os.path.join(feature_dir, \"ecfp4_disk\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        print(\"ECFP4 Featurization - to DiskDataset\")\n",
    "\n",
    "        ## creating a deepchem dataset with featurized smiles\n",
    "        featurizer = dc.feat.CircularFingerprint(size=nBits, radius=radius)\n",
    "        feats = featurizer.featurize(data.smiles)\n",
    "        dataset = dc.data.DiskDataset.from_numpy(\n",
    "            feats,\n",
    "            data.rt,\n",
    "            tasks=[\"RT_sec\"],\n",
    "            ids=data.smiles,\n",
    "            data_dir=os.path.join(target_dir, \"all_data\"),\n",
    "        )\n",
    "\n",
    "        target_dir = os.path.join(feature_dir, \"ecfp4_csv\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        print(\"ECFP4 Featurization - to .CSV\")\n",
    "        feats_df = dataset.to_dataframe()\n",
    "        feats_df.to_csv(os.path.join(target_dir, \"all_data.csv\"), index=False)\n",
    "\n",
    "        NA_alert = feats_df.isna().any().any()\n",
    "        if NA_alert:\n",
    "            print(\"*** NA values in:\", target_dir)\n",
    "\n",
    "    ###* RDKit Calculations\n",
    "    if \"rdkit\" in feature_list:\n",
    "        target_dir = os.path.join(feature_dir, \"rdkit_disk\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        print(\"RDKit Descriptors - to diskdataset\")\n",
    "        ## creating a deepchem dataset with featurized smiles\n",
    "        featurizer = dc.feat.RDKitDescriptors(is_normalized=True, use_bcut2d=False)\n",
    "        feats = featurizer.featurize(data.smiles)\n",
    "        dataset = dc.data.DiskDataset.from_numpy(\n",
    "            feats,\n",
    "            data.rt,\n",
    "            tasks=[\"RT_sec\"],\n",
    "            ids=data.smiles,\n",
    "            data_dir=os.path.join(target_dir, \"all_data\"),\n",
    "        )\n",
    "\n",
    "        target_dir = os.path.join(feature_dir, \"rdkit_csv\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        print(\"RDKit Descriptors - to .CSV\")\n",
    "        feats_df = dataset.to_dataframe()\n",
    "        feats_df = feats_df.fillna(0)\n",
    "\n",
    "        feats_df.to_csv(os.path.join(target_dir, \"all_data.csv\"), index=False)\n",
    "\n",
    "        NA_alert = feats_df.isna().any().any()\n",
    "        if NA_alert:\n",
    "            print(\"*** NA values in:\", target_dir)\n",
    "\n",
    "    ###* MolGraphConv\n",
    "    if \"molgraphconv\" in feature_list:\n",
    "        target_dir = os.path.join(feature_dir, \"molgraphconv\")\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        print(\"MolGraphConv Feat - to diskdataset\")\n",
    "        ## creating a deepchem dataset with featurized smiles\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        feats = featurizer.featurize(data.smiles)\n",
    "        dataset = dc.data.DiskDataset.from_numpy(\n",
    "            feats,\n",
    "            data.rt,\n",
    "            tasks=[\"RT_sec\"],\n",
    "            ids=data.smiles,\n",
    "            data_dir=os.path.join(target_dir, \"all_data\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8192029e-061b-4b1c-9ad1-19dc241ea4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data splitting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def data_splitter(path_to_data, destination_dir, number_of_splits=5, seed=42):\n",
    "    ##* Loading data into diskDataset and using scaffold-split\n",
    "    data = pd.read_csv(path_to_data)\n",
    "\n",
    "    dataset = dc.data.NumpyDataset(X=data.id, y=data.rt, ids=data.smiles)\n",
    "\n",
    "    splitter = dc.splits.ScaffoldSplitter()\n",
    "\n",
    "    train_dataset, test_dataset = splitter.train_test_split(\n",
    "        dataset=dataset, frac_train=0.9, seed=seed\n",
    "    )\n",
    "\n",
    "    ##* saving test and train splits as .CSV\n",
    "    train_df = train_dataset.to_dataframe()\n",
    "    train_df = train_df.rename(columns={\"X\": \"id\", \"y\": \"rt\", \"ids\": \"smiles\"})\n",
    "    train_df.to_csv(os.path.join(destination_dir, \"train_df.csv\"), index=False)\n",
    "\n",
    "    test_df = test_dataset.to_dataframe()\n",
    "    test_df = test_df.rename(columns={\"X\": \"id\", \"y\": \"rt\", \"ids\": \"smiles\"})\n",
    "    test_df.to_csv(os.path.join(destination_dir, \"test_df.csv\"), index=False)\n",
    "\n",
    "    ##* Preparing Cross validation splits\n",
    "    target_dir = os.path.join(destination_dir, \"cv_splits\")\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    cv_splits = splitter.k_fold_split(dataset=train_dataset, k=number_of_splits)\n",
    "\n",
    "    for i, k_split in enumerate(cv_splits):\n",
    "        train_k_split = k_split[0]\n",
    "        train_k_split = train_k_split.to_dataframe()\n",
    "        train_k_split = train_k_split.rename(\n",
    "            columns={\"X\": \"id\", \"y\": \"rt\", \"ids\": \"smiles\"}\n",
    "        )\n",
    "        train_k_split.to_csv(\n",
    "            os.path.join(target_dir, f\"train_{i}_split.csv\"), index=False\n",
    "        )\n",
    "\n",
    "        valid_k_split = k_split[1]\n",
    "        valid_k_split = valid_k_split.to_dataframe()\n",
    "        valid_k_split = valid_k_split.rename(\n",
    "            columns={\"X\": \"id\", \"y\": \"rt\", \"ids\": \"smiles\"}\n",
    "        )\n",
    "        valid_k_split.to_csv(\n",
    "            os.path.join(target_dir, f\"valid_{i}_split.csv\"), index=False\n",
    "        )\n",
    "\n",
    "\n",
    "def feature_splitter_csv(\n",
    "    path_to_features, path_to_splits, save_folder_name=\"data_splits\"\n",
    "):\n",
    "    ##*  collecting the needed paths for the features\n",
    "\n",
    "    csv_feature_paths = []\n",
    "    csv_feature_subdirs = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(path_to_features):\n",
    "        for file in files:\n",
    "            # csv-based features\n",
    "            if file.startswith(\"all_data\") and file.endswith(\".csv\"):\n",
    "                csv_feature_paths.append(os.path.join(subdir, file))\n",
    "                csv_feature_subdirs.append(subdir)\n",
    "\n",
    "    ##* getting the paths for the premade-splits\n",
    "\n",
    "    split_paths = []\n",
    "    output_paths = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(path_to_splits):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                split_paths.append(os.path.join(subdir, file))\n",
    "                output_paths.append(file)\n",
    "\n",
    "    ##* walking through each .CSV feature dir\n",
    "\n",
    "    for i, path in enumerate(csv_feature_paths):\n",
    "        features = pd.read_csv(path).rename(columns={\"y\": \"rt\", \"ids\": \"id\"})\n",
    "\n",
    "        # two types of feature output: Deepchem generated\n",
    "        if \"w\" in features:\n",
    "            features = features.drop([\"w\", \"rt\"], axis=1)\n",
    "\n",
    "        # or Chemaxon (cxcalc) generated\n",
    "        elif \"logD[pH=4.5]\" in features:\n",
    "            features = features.drop([\"smiles\", \"rt\"], axis=1)\n",
    "\n",
    "        ##* for each feature type, the features are split according to the premade_splits\n",
    "        for j, split in enumerate(split_paths):\n",
    "            split = pd.read_csv(split_paths[j])\n",
    "            merged_split = pd.concat([split, features], axis=1, join=\"inner\")\n",
    "\n",
    "            save_dir = os.path.join(csv_feature_subdirs[i], save_folder_name)  #!\n",
    "            if not os.path.exists(save_dir):  #!\n",
    "                os.makedirs(save_dir)  #!\n",
    "\n",
    "            labels_split = merged_split[[\"smiles\", \"rt\"]]\n",
    "            labels_split.to_csv(\n",
    "                os.path.join(save_dir, (\"labels_\" + output_paths[j])), index=False\n",
    "            )\n",
    "\n",
    "            feature_split = merged_split.drop([\"id\", \"rt\", \"smiles\"], axis=1)\n",
    "            if \"w\" in feature_split:\n",
    "                feature_split = feature_split.drop([\"w\"], axis=1)\n",
    "            feature_split.to_csv(\n",
    "                os.path.join(save_dir, (\"features_\" + output_paths[j])), index=False\n",
    "            )\n",
    "\n",
    "\n",
    "def feature_splitter_diskdatasets(\n",
    "    path_to_features, path_to_splits, save_folder_name=\"data_splits\"\n",
    "):\n",
    "    ##*  collecting the needed paths for the features\n",
    "\n",
    "    disk_feature_subdirs = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(path_to_features):\n",
    "        for file in files:\n",
    "            # csv-based features\n",
    "            if file.endswith(\".gzip\") and (\"all_data\" in subdir):\n",
    "                disk_feature_subdirs.append(subdir)\n",
    "\n",
    "    ##* getting the paths for the premade-splits\n",
    "\n",
    "    split_paths = []\n",
    "    output_paths = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(path_to_splits):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                split_paths.append(os.path.join(subdir, file))\n",
    "                output_paths.append(file)\n",
    "\n",
    "    for i, path in enumerate(disk_feature_subdirs):\n",
    "        path = os.path.abspath(path)\n",
    "        ##* loading all datapoints for the specific feature set\n",
    "        dataset = dc.data.DiskDataset(data_dir=path)\n",
    "        array = dataset.ids\n",
    "\n",
    "        ##* for each feature type, the features are split according to the premade_splits\n",
    "        for j, split in enumerate(split_paths):\n",
    "            # loading split dataset and extracts IDs\n",
    "            split = pd.read_csv(split_paths[j])\n",
    "            df_ids = split[\"smiles\"].values\n",
    "\n",
    "            # matches with the array to find indexes of datapoints present in current split\n",
    "            matches = np.where(np.isin(array, df_ids))[0]\n",
    "\n",
    "            # defining a folder name based on split name\n",
    "            split_name = re.search(r\"(.*).csv$\", output_paths[j])[1]\n",
    "            feat_dir = re.search(r\"features\\\\(.*)\\\\all_data\", path)[1]\n",
    "            output_dir = os.path.join(\n",
    "                path_to_features, feat_dir, save_folder_name, split_name\n",
    "            )\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            # using the deepchem .select() function to subset the dataset according to matches\n",
    "            dataset.select(matches, select_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5421448-0241-497b-8dc2-81447540f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e424a25-1797-4c6e-bcc4-d1a4dd8895a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:21:49] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[23:21:49] ERROR: Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:49] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:49] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:49] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:49] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:49] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:49] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 23 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 23 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:50] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[23:21:50] ERROR: Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 26 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 26 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 11 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 11 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 26 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 26 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:51] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[23:21:51] ERROR: Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:52] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:52] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 26 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 26 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 23 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 23 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:53] Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "[23:21:53] ERROR: Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 17 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 20 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:54] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:54] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[23:21:55] ERROR: Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:55] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:21:55] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:21:55] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "[23:21:55] ERROR: Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:21:55] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:21:55] WARNING:  Problems/mismatches: Mobile-H( Hydrogens: Locations or number; Mobile-H groups: Attachment points, Number)\n",
      "\n",
      "[23:22:00] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:22:00] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:00] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "[23:22:00] ERROR: Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 23 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 23 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 22 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:01] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[23:22:01] ERROR: Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:02] Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "[23:22:02] ERROR: Explicit valence for atom # 24 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:05] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:22:05] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:08] Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "[23:22:08] ERROR: Explicit valence for atom # 25 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:11] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[23:22:11] ERROR: Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:12] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:22:12] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:13] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[23:22:13] ERROR: Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "\n",
      "[23:22:34] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[23:22:34] ERROR: Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to:  ../data/metlin_smrt/sample_dataset_5000_datapoints.csv \n",
      "\n",
      "     id                                             smiles     rt\n",
      "1  3505  COC(=O)N1CCN(C(=O)Cc2ccc(Cl)c(Cl)c2)[C@H](CN2C...  687.8\n",
      "2  2159    CCN1CCC[C@@H]1CN=C(O)c1cc(S(=O)(=O)CC)c(N)cc1OC  590.7\n",
      "3  1340                                 Oc1cccc2c(O)nccc12  583.6\n"
     ]
    }
   ],
   "source": [
    "path_to_data = get_METLIN_data(sampling = 5000, only_retained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90bfdfe6-f323-4cce-951f-0161486e8ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/metlin_smrt/sample_dataset_5000_datapoints.csv\n"
     ]
    }
   ],
   "source": [
    "print(path_to_data)\n",
    "path = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d64faeb-cf7b-4bfc-aca8-f32f8e09240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_features = 'features/' \n",
    "if not os.path.exists(path_to_features):\n",
    "    os.makedirs(path_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3dd92ac-7f80-453f-b15a-6a93d6dc9e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECFP4 Featurization - to DiskDataset\n",
      "ECFP4 Featurization - to .CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDKit Descriptors - to diskdataset\n",
      "RDKit Descriptors - to .CSV\n",
      "MolGraphConv Feat - to diskdataset\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['logD', \n",
    "                'ecfp4', \n",
    "                'rdkit', \n",
    "                'molgraphconv'\n",
    "                ]\n",
    "\n",
    "get_features(path, path_to_features, feature_list)\n",
    "             # path_to_chemaxon_licence= path_to_chemaxon_licence, #I need licenses for this\n",
    "             # path_to_chemaxon_tools = path_to_chemaxon_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4a637a1-5249-4ea4-bc2b-cc8f61748a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_splits = 'data_splits/'\n",
    "if not os.path.exists(path_to_splits):\n",
    "    os.makedirs(path_to_splits)\n",
    "\n",
    "data_splitter(path, path_to_splits, number_of_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49aad698-ad0a-4414-9688-55040efe925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "def xgboost_hyperopt(\n",
    "    model_check_point, X_train, y_train, X_valid, y_valid, epochs=100, iterations=20\n",
    "):\n",
    "    ##* writing objective function\n",
    "\n",
    "    def objective(params):\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=epochs,  #Number of trees (epochs)\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=int(params[\"max_depth\"]),\n",
    "            subsample=params[\"subsample\"],\n",
    "            gamma=params[\"gamma\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"],\n",
    "            min_child_weight=int(params[\"min_child_weight\"]),\n",
    "            reg_alpha=params[\"reg_alpha\"],\n",
    "            reg_lambda=params[\"reg_lambda\"],\n",
    "            eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "\n",
    "        # Train the model on the training data with early stopping\n",
    "        eval_set = [(X_valid, y_valid)]\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Get the best iteration based on early stopping\n",
    "        best_iteration = model.best_iteration\n",
    "\n",
    "        # Score the model on the validation data using the best iteration\n",
    "        y_pred = model.predict(X_valid, iteration_range = (0, best_iteration + 1))\n",
    "        mse = mean_squared_error(y_valid, y_pred)\n",
    "        score = mse\n",
    "    \n",
    "        return score\n",
    "\n",
    "    ##* Define the search space for hyperparameters\n",
    "    space = {\n",
    "        \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n",
    "        \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.7, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0, 1),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.7, 1.0),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 10, 1),\n",
    "        \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(1e-10), np.log(1.0)),\n",
    "        \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(1e-10), np.log(1.0)),\n",
    "    }\n",
    "\n",
    "    ##* running hyperopt trials\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=iterations,  # Number of iterations\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    ##* outputting trial results\n",
    "\n",
    "    print(\"Best Parameters: {}\".format(best))\n",
    "\n",
    "    return best\n",
    "\n",
    "def xgboost_retrain_test(\n",
    "    best_params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    model_check_point,\n",
    "    epochs=100,\n",
    "):\n",
    "    # Create an XGBoost model with the best hyperparameters\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=epochs,  # Choose an appropriate number of trees (epochs)\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        max_depth=int(best_params[\"max_depth\"]),\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        gamma=best_params[\"gamma\"],\n",
    "        colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "        min_child_weight=int(best_params[\"min_child_weight\"]),\n",
    "        reg_alpha=best_params[\"reg_alpha\"],\n",
    "        reg_lambda=best_params[\"reg_lambda\"],\n",
    "        eval_metric=\"rmse\",\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Train the model on the training data with early stopping\n",
    "    eval_set = [(X_valid, y_valid)]\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=eval_set,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Get the best iteration based on early stopping\n",
    "    best_iteration = model.best_iteration\n",
    "\n",
    "    # ##* saving the model\n",
    "    # file_name = os.path.join(model_check_point, \"xgboost_model.pkl\")\n",
    "    # # save\n",
    "    # pickle.dump(best_iteration, open(file_name, \"wb\"))\n",
    "\n",
    "    # Score the model on the validation data using the best iteration\n",
    "    y_pred = model.predict(X_test, iteration_range = (0, best_iteration + 1))\n",
    "    y_true = y_test\n",
    "\n",
    "    r2_metric = r2_score(y_true, y_pred)\n",
    "    print(f\"R2 score: {r2_metric}\")\n",
    "\n",
    "    mse_score = mean_squared_error(y_true, y_pred, squared=True)\n",
    "    print(f\"MSE Score: {mse_score}\")\n",
    "\n",
    "    rmse_score = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    print(f\"RMSE Score: {rmse_score}\")\n",
    "\n",
    "    mae_score = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"MAE Score: {mae_score}\")\n",
    "\n",
    "    score_dict = {\n",
    "        \"R2 Score\": r2_metric,\n",
    "        \"MSE Score\": mse_score,\n",
    "        \"RMSE Score\": rmse_score,\n",
    "        \"MAE Score\": mae_score,\n",
    "    }\n",
    "\n",
    "    # with open((model_check_point + \"/test_scores.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(score_dict, f, default=int)\n",
    "\n",
    "    df = pd.DataFrame({\"actual_RT\": y_true, \"pred_RT\": y_pred})\n",
    "    # print(df.head())\n",
    "    # pred_df_path = model_check_point + \"/test_preds.csv\"\n",
    "    # df.to_csv(pred_df_path, index=False)\n",
    "    # print(f\"saved predictions to {pred_df_path}\")\n",
    "\n",
    "    print(\"\\n*** Hyperparameter Optimization is done ***\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7122cfa7-9a86-480f-abf5-94c30048aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* splitting the .CSV-based features according to the data splits\n",
    "feature_splitter_csv(path_to_features, path_to_splits, save_folder_name = 'cv_splits')\n",
    "\n",
    "#* splitting the diskDataset-based features according to the data splits\n",
    "# feature_splitter_diskdatasets(path_to_features,path_to_splits, save_folder_name = 'cv_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3417bd9-19de-4922-9b73-053af2f9a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_splits():\n",
    "    for i in range(5):\n",
    "        print(f\"Iteration {i}\")\n",
    "        y_train = pd.read_csv(f'features/rdkit_csv/cv_splits/labels_train_{i}_split.csv')[\"rt\"]\n",
    "        X_train = pd.read_csv(f'features/rdkit_csv/cv_splits/features_train_{i}_split.csv')\n",
    "        new_column_names = [\"X\" + str(i) for i in range(len(X_train.columns))]\n",
    "        X_train.columns = new_column_names\n",
    "        \n",
    "        y_valid = pd.read_csv(f'features/rdkit_csv/cv_splits/labels_valid_{i}_split.csv')[\"rt\"]\n",
    "        X_valid = pd.read_csv(f\"features/rdkit_csv/cv_splits/features_valid_{i}_split.csv\")\n",
    "        new_column_names = [\"X\" + str(i) for i in range(len(X_valid.columns))]\n",
    "        X_valid.columns = new_column_names\n",
    "    \n",
    "        y_test = pd.read_csv('features/rdkit_csv/cv_splits/labels_test_df.csv')[\"rt\"]\n",
    "        X_test = pd.read_csv('features/rdkit_csv/cv_splits/features_test_df.csv')\n",
    "        new_column_names = [\"X\" + str(i) for i in range(len(X_test.columns))]\n",
    "        X_test.columns = new_column_names\n",
    "\n",
    "        epochs = 200\n",
    "        iterations = 100\n",
    "        \n",
    "        best_params = xgboost_hyperopt(\n",
    "            'model_check_point',\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_valid,\n",
    "            y_valid,\n",
    "            epochs=epochs,\n",
    "            iterations=iterations,\n",
    "        )\n",
    "\n",
    "        xgboost_retrain_test(best_params, X_train, y_train, X_valid, y_valid, X_test, y_test, \"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecb5bc4c-af5c-4b42-b13c-043c21fa0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.61trial/s, best loss: 33034.20281000577]\n",
      "Best Parameters: {'colsample_bytree': 0.8132748059317126, 'gamma': 0.08154875888669734, 'learning_rate': 0.2395234166432407, 'max_depth': 9.0, 'min_child_weight': 1.0, 'reg_alpha': 3.212188066985545e-08, 'reg_lambda': 0.00016580822974435183, 'subsample': 0.7739758547709046}\n",
      "R2 score: -0.07776937071170953\n",
      "MSE Score: 25415.930323286742\n",
      "RMSE Score: 159.42374454041263\n",
      "MAE Score: 125.98740350341797\n",
      "\n",
      "*** Hyperparameter Optimization is done ***\n",
      "\n",
      "Iteration 1\n",
      "  0%|                                                                                                                                                                 | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.69trial/s, best loss: 28554.265390076274]\n",
      "Best Parameters: {'colsample_bytree': 0.797414507459184, 'gamma': 0.20400219391862148, 'learning_rate': 0.2976297241708892, 'max_depth': 9.0, 'min_child_weight': 2.0, 'reg_alpha': 0.00022676218262877471, 'reg_lambda': 0.008386262976650318, 'subsample': 0.850528864662465}\n",
      "R2 score: -0.07406525659445129\n",
      "MSE Score: 25328.57999688846\n",
      "RMSE Score: 159.14955229873712\n",
      "MAE Score: 126.53177893066407\n",
      "\n",
      "*** Hyperparameter Optimization is done ***\n",
      "\n",
      "Iteration 2\n",
      "  0%|                                                                                                                                                                 | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:23<00:00,  4.20trial/s, best loss: 25565.974063150916]\n",
      "Best Parameters: {'colsample_bytree': 0.9447058035104805, 'gamma': 0.565626587083945, 'learning_rate': 0.08332840847389344, 'max_depth': 5.0, 'min_child_weight': 1.0, 'reg_alpha': 0.5176653336202008, 'reg_lambda': 7.737759814870895e-08, 'subsample': 0.8847036289492267}\n",
      "R2 score: -0.09574926082613833\n",
      "MSE Score: 25839.93164192406\n",
      "RMSE Score: 160.74803775450593\n",
      "MAE Score: 129.07367041015627\n",
      "\n",
      "*** Hyperparameter Optimization is done ***\n",
      "\n",
      "Iteration 3\n",
      "  0%|                                                                                                                                                                 | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.71trial/s, best loss: 30336.734060807587]\n",
      "Best Parameters: {'colsample_bytree': 0.8472500808261842, 'gamma': 0.5698275731884752, 'learning_rate': 0.14788854242963018, 'max_depth': 9.0, 'min_child_weight': 1.0, 'reg_alpha': 1.8179731438332954e-08, 'reg_lambda': 5.801003113520564e-09, 'subsample': 0.8045812962504085}\n",
      "R2 score: -0.08334674289243349\n",
      "MSE Score: 25547.455774449558\n",
      "RMSE Score: 159.8357149527275\n",
      "MAE Score: 127.56905364990234\n",
      "\n",
      "*** Hyperparameter Optimization is done ***\n",
      "\n",
      "Iteration 4\n",
      "  0%|                                                                                                                                                                 | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.47trial/s, best loss: 27773.91906430625]\n",
      "Best Parameters: {'colsample_bytree': 0.9994501862770774, 'gamma': 0.6094756280429889, 'learning_rate': 0.055971223194102096, 'max_depth': 7.0, 'min_child_weight': 6.0, 'reg_alpha': 1.0988341711456445e-10, 'reg_lambda': 0.34640477612153725, 'subsample': 0.8987448049385149}\n",
      "R2 score: -0.06990556805754577\n",
      "MSE Score: 25230.486325929203\n",
      "RMSE Score: 158.84107254085512\n",
      "MAE Score: 126.49076300048827\n",
      "\n",
      "*** Hyperparameter Optimization is done ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b0cdb24-cdac-4569-8109-7b17b5e92554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: -1.3166329749700343\n",
      "MSE Score: 54630.78082984083\n",
      "RMSE Score: 233.73228452620924\n",
      "MAE Score: 186.5319760131836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/home/Aarav/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_params = {'colsample_bytree': 0.9994501862770774, 'gamma': 0.6094756280429889, 'learning_rate': 0.055971223194102096, 'max_depth': 7.0, 'min_child_weight': 6.0, 'reg_alpha': 1.0988341711456445e-10, 'reg_lambda': 0.34640477612153725, 'subsample': 0.8987448049385149}\n",
    "model = xgb.XGBRegressor(\n",
    "        n_estimators=200,  # Choose an appropriate number of trees (epochs)\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        max_depth=int(best_params[\"max_depth\"]),\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        gamma=best_params[\"gamma\"],\n",
    "        colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "        min_child_weight=int(best_params[\"min_child_weight\"]),\n",
    "        reg_alpha=best_params[\"reg_alpha\"],\n",
    "        reg_lambda=best_params[\"reg_lambda\"],\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "x_train_df = pd.read_csv('features/rdkit_csv/cv_splits/features_train_df.csv')\n",
    "x_test_df = pd.read_csv('features/rdkit_csv/cv_splits/features_test_df.csv')\n",
    "\n",
    "y_train_df = pd.read_csv('features/rdkit_csv/cv_splits/labels_train_df.csv')['rt']\n",
    "y_test_df = pd.read_csv('features/rdkit_csv/cv_splits/labels_test_df.csv')['rt']\n",
    "\n",
    "model.fit(\n",
    "        x_train_df,\n",
    "        y_train_df,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "# Get the best iteration based on early stopping\n",
    "# best_iteration = model.best_iteration\n",
    "\n",
    "# ##* saving the model\n",
    "# file_name = os.path.join(model_check_point, \"xgboost_model.pkl\")\n",
    "# # save\n",
    "# pickle.dump(best_iteration, open(file_name, \"wb\"))\n",
    "\n",
    "# Score the model on the validation data using the best iteration\n",
    "y_pred = model.predict(x_test_df)\n",
    "y_true = y_test_df\n",
    "\n",
    "r2_metric = r2_score(y_true, y_pred)\n",
    "print(f\"R2 score: {r2_metric}\")\n",
    "\n",
    "mse_score = mean_squared_error(y_true, y_pred, squared=True)\n",
    "print(f\"MSE Score: {mse_score}\")\n",
    "\n",
    "rmse_score = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(f\"RMSE Score: {rmse_score}\")\n",
    "\n",
    "mae_score = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE Score: {mae_score}\")\n",
    "\n",
    "score_dict = {\n",
    "    \"R2 Score\": r2_metric,\n",
    "    \"MSE Score\": mse_score,\n",
    "    \"RMSE Score\": rmse_score,\n",
    "    \"MAE Score\": mae_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "827f6c0c-c2ae-424b-9fbc-092519248a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mgn(train_dir, val_dir, test_dir, callback_intervals, batch_size, epochs, iterations):\n",
    "    #     print(\n",
    "    #     \"\\n** This is the AttentiveFP Regressor, for retention-time prediction, RMSE optimized **\\n\"\n",
    "    # )\n",
    "    \n",
    "    ############################################################################\n",
    "    # Reading featurized, and split data\n",
    "    ############################################################################\n",
    "    \n",
    "    train_dataset = dc.data.DiskDataset(data_dir=args.train_dir)\n",
    "    valid_dataset = dc.data.DiskDataset(data_dir=args.val_dir)\n",
    "    test_dataset = dc.data.DiskDataset(data_dir=args.test_dir)\n",
    "\n",
    "    \n",
    "    ############################################################################\n",
    "    # Defining Search Space\n",
    "    ############################################################################\n",
    "    \n",
    "    search_space = {\n",
    "        ##* AttentiveFP\n",
    "        \"num_layers\": hp.randint(\"num_layers\", high=6, low=1),\n",
    "        \"graph_feat_size\": hp.randint(\"graph_feat_size\", high=300, low=30),\n",
    "        \"dropout\": hp.uniform(\"dropout\", high=0.5, low=0.0),\n",
    "        \"learning_rate\": hp.loguniform(\n",
    "            \"learning_rate\", high=(np.log(0.1)), low=(np.log(0.0001))\n",
    "        ),\n",
    "        \"weight_decay_penalty\": hp.loguniform(\n",
    "            \"weight_decay_penalty\", high=(np.log(0.01)), low=(np.log(0.00001))\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    ############################################################################\n",
    "    # Model training\n",
    "    ############################################################################\n",
    "    \n",
    "    metric = dc.metrics.Metric(dc.metrics.mean_squared_error)\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batchsize\n",
    "    validation_interval = args.callback_intervals\n",
    "    n_tasks = len(train_dataset.tasks)\n",
    "    \n",
    "    \n",
    "    def fm(search_args):\n",
    "        model = dc.models.AttentiveFPModel(\n",
    "            n_tasks=n_tasks,\n",
    "            mode=\"regression\",\n",
    "            model_dir=model_check_point,\n",
    "            batch_size=batch_size,\n",
    "            ##* AttentiveFP Search Arguments\n",
    "            num_layers=search_args[\"num_layers\"],\n",
    "            graph_feat_size=search_args[\"graph_feat_size\"],\n",
    "            dropout=search_args[\"dropout\"],\n",
    "            learning_rate=search_args[\"learning_rate\"],\n",
    "            weight_decay=search_args[\"weight_decay_penalty\"],\n",
    "        )\n",
    "    \n",
    "        # validation callback that saves the best checkpoint, i.e the one with the maximum score.\n",
    "        callbacks = dc.models.ValidationCallback(\n",
    "            valid_dataset,\n",
    "            validation_interval,\n",
    "            [metric],\n",
    "            save_dir=model_check_point,\n",
    "            # transformers=transformers,\n",
    "            save_on_minimum=True,\n",
    "        )\n",
    "    \n",
    "        model.fit(train_dataset, nb_epoch=epochs, callbacks=callbacks)\n",
    "    \n",
    "        # restoring the best checkpoint and passing the negative of its validation score to be minimized.\n",
    "        model.restore(model_dir=model_check_point)\n",
    "        valid_score = model.evaluate(valid_dataset, [metric])\n",
    "        return valid_score[\"mean_squared_error\"]\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # Hyperparameter optimization trials\n",
    "    ############################################################################\n",
    "    \n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fm,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=args.iterations,\n",
    "        trials=trials,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ##* outputting trial results\n",
    "    \n",
    "    print(\"Best Parameters: {}\".format(best))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # Evaluating on test data\n",
    "    ############################################################################\n",
    "    n_tasks = len(test_dataset.tasks)\n",
    "    model = dc.models.AttentiveFPModel(\n",
    "        n_tasks=n_tasks,\n",
    "        mode=\"regression\",\n",
    "        model_dir=model_check_point,\n",
    "        batch_size=batch_size,\n",
    "        ##* AttentiveFP Search Arguments\n",
    "        num_layers=best[\"num_layers\"],\n",
    "        graph_feat_size=best[\"graph_feat_size\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        learning_rate=best[\"learning_rate\"],\n",
    "        weight_decay=best[\"weight_decay_penalty\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"*** Training model based on best parameters ***\")\n",
    "    model.fit(train_dataset, nb_epoch=epochs)\n",
    "    \n",
    "    \n",
    "    print(\"making predictions on test...\")\n",
    "    preds = model.predict(test_dataset)\n",
    "    preds = np.squeeze(preds)\n",
    "    \n",
    "    df = pd.DataFrame({\"actual_RT\": test_dataset.y, \"pred_RT\": preds})\n",
    "    y_true = df[\"actual_RT\"]\n",
    "    y_pred = df[\"pred_RT\"]\n",
    "    \n",
    "    \n",
    "    r2_score = r2_score(y_true, y_pred)\n",
    "    print(f\"R2 score: {r2_score}\")\n",
    "    \n",
    "    mse_score = mean_squared_error(y_true, y_pred, squared=True)\n",
    "    print(f\"MSE Score: {mse_score}\")\n",
    "    \n",
    "    rmse_score = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    print(f\"RMSE Score: {rmse_score}\")\n",
    "    \n",
    "    mae_score = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"MAE Score: {mae_score}\")\n",
    "    \n",
    "    \n",
    "    score_dict = {\n",
    "        \"R2 Score\": r2_score,\n",
    "        \"MSE Score\": mse_score,\n",
    "        \"RMSE Score\": rmse_score,\n",
    "        \"MAE Score\": mae_score,\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(\"\\n*** Hyperparameter Optimization is done ***\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a2b85-26d2-46e3-b35f-8bf2042019d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_splits():\n",
    "#     for i in range(5):\n",
    "#         print(f\"Iteration {i}\")\n",
    "\n",
    "#         epochs = 200\n",
    "#         iterations = 100\n",
    "#         batch = 30\n",
    "#         callback = 1000\n",
    "\n",
    "#         train_dir = f'features/rdkit_csv/cv_splits/labels_train_{i}_split.csv'\n",
    "        \n",
    "#         best_params = mgn(\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
